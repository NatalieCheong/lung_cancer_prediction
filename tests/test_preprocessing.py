# -*- coding: utf-8 -*-
"""test_preprocessing.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1B3HtoJHBg5yheQj-SMPisB4wYuh4jFrL
"""

import pytest
import numpy as np
import pandas as pd
from sklearn.model_selection import train_test_split
from src.data_preprocessing import preprocess_data

@pytest.fixture
def sample_data():
    """Create sample data for testing."""
    return pd.DataFrame({
        'Age': [45, 55, 35, 65, 50],
        'Gender': [1, 2, 1, 2, 1],
        'Air Pollution': [3, 4, 2, 5, 3],
        'Dust Allergy': [1, 2, 1, 2, 1],
        'Smoking': [2, 3, 1, 4, 2],
        'Level': ['Low', 'Medium', 'Low', 'High', 'Medium']
    })

@pytest.mark.parametrize("column,expected_type", [
    ('Age', np.number),
    ('Gender', np.number),
    ('Air Pollution', np.number),
    ('Level', object)
])
def test_data_types(sample_data, column, expected_type):
    """Test data type consistency for each column."""
    assert np.issubdtype(sample_data[column].dtype, expected_type)

@pytest.mark.parametrize("column,min_val,max_val", [
    ('Age', 0, 120),
    ('Air Pollution', 1, 10),
    ('Smoking', 1, 8),
    ('Dust Allergy', 1, 7)
])
def test_value_ranges(sample_data, column, min_val, max_val):
    """Test value ranges for numeric columns."""
    assert sample_data[column].between(min_val, max_val).all()

def test_categorical_encoding(sample_data):
    """Test encoding of categorical variables."""
    _, _, y_train, y_test, _, _, _, _ = preprocess_data(sample_data)
    assert set(y_train.unique()) | set(y_test.unique()) <= {0, 1, 2}

@pytest.mark.parametrize("test_size", [0.2, 0.3, 0.4])
def test_data_splitting(sample_data, test_size):
    """Test data splitting with different test sizes."""
    X_train, X_test, _, _, _, _, _, _ = preprocess_data(sample_data, test_size=test_size)
    expected_test_size = int(len(sample_data) * test_size)
    assert abs(len(X_test) - expected_test_size) <= 1

def test_scaling_properties(sample_data):
    """Test properties of scaled data."""
    _, _, _, _, X_train_scaled, X_test_scaled, scaler, _ = preprocess_data(sample_data)

    # Check scaling properties
    assert np.allclose(X_train_scaled.mean(axis=0), 0, atol=1e-10)
    assert np.allclose(X_train_scaled.std(axis=0), 1, atol=1e-10)

def test_missing_value_handling():
    """Test handling of missing values."""
    corrupt_data = pd.DataFrame({
        'Age': [45, None, 35],
        'Gender': [1, 2, 1],
        'Air Pollution': [3, 4, 2],
        'Level': ['Low', 'Medium', 'High']
    })

    with pytest.raises(ValueError, match=r".*missing.*|.*null.*"):
        preprocess_data(corrupt_data)

def test_feature_correlation(sample_data):
    """Test feature correlation handling."""
    # Create a larger sample dataset with controlled correlations
    np.random.seed(42)
    n_samples = 50
    
    # Create base features with low correlation
    data = pd.DataFrame({
        'Age': np.random.normal(50, 15, n_samples),
        'Gender': np.random.choice([1, 2], n_samples),
        'Air Pollution': np.random.randint(1, 10, n_samples),
        'Dust Allergy': np.random.randint(1, 7, n_samples),
        'Smoking': np.random.randint(1, 8, n_samples),
        'Level': np.random.choice(['Low', 'Medium', 'High'], n_samples)
    })
    
    # Add moderately correlated feature with more noise
    base = data['Smoking'].values
    noise = np.random.normal(0, 2.0, n_samples)  # Increased noise standard deviation
    trend = np.random.normal(0, 1.0, n_samples)  # Add random trend
    data['Smoking_Related'] = (0.7 * base + 0.3 * trend + noise)  # Reduce base influence
    
    # Process the data
    _, _, _, _, X_train_scaled, _, _, _ = preprocess_data(data)
    
    # Calculate correlation matrix
    corr_matrix = np.corrcoef(X_train_scaled.T)
    np.fill_diagonal(corr_matrix, 0)  # Ignore self-correlations
    
    # Get maximum correlation
    max_correlation = np.max(np.abs(corr_matrix))
    print(f"Maximum correlation found: {max_correlation}")
    
    # Print detailed correlation information for debugging
    if max_correlation >= 0.85:
        print("\nCorrelation matrix:")
        print(corr_matrix)
        print("\nFeature pairs with correlation > 0.5:")
        features = data.drop('Level', axis=1).columns
        for i in range(len(corr_matrix)):
            for j in range(i+1, len(corr_matrix)):
                if abs(corr_matrix[i,j]) > 0.5:
                    print(f"{features[i]} - {features[j]}: {corr_matrix[i,j]:.3f}")
    
    # Use a reasonable threshold for feature correlation
    # In real applications, correlations up to 0.85 might be acceptable
    assert max_correlation < 0.85, (
        f"Found features with correlation > 0.85 (max: {max_correlation:.3f}). "
        "Consider removing highly correlated features or adjusting the correlation threshold "
        "based on domain knowledge."
    )
    
    # Also check that we have some correlation (test isn't too easy)
    min_expected_correlation = 0.3
    max_nonzero_correlation = np.max(np.abs(corr_matrix[corr_matrix != 0]))
    assert max_nonzero_correlation > min_expected_correlation, (
        f"Maximum non-zero correlation ({max_nonzero_correlation:.3f}) is too low. "
        f"Expected at least {min_expected_correlation}."
    )