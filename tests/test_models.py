# -*- coding: utf-8 -*-
"""test_models.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1573Wez04QKVulLL7k55i51NpoSHOwyFf
"""

# tests/test_models.py

import pytest
import numpy as np
import pandas as pd
from sklearn.datasets import make_classification
from src.model_training import train_models, evaluate_models

@pytest.fixture
def sample_data():
    """Create sample data for model testing."""
    X, y = make_classification(
        n_samples=100,
        n_features=20,
        n_informative=15,
        n_redundant=5,
        n_classes=3,
        random_state=42
    )
    # Split into train and test
    train_idx = range(80)
    test_idx = range(80, 100)

    return {
        'X_train': X[train_idx],
        'X_test': X[test_idx],
        'y_train': y[train_idx],
        'y_test': y[test_idx]
    }

def test_train_models(sample_data):
    """Test model training function."""
    # Train models
    models = train_models(sample_data['X_train'], sample_data['y_train'])

    # Check if all models are trained
    assert 'SVM' in models
    assert 'KNN' in models
    assert 'Decision Tree' in models
    assert 'Random Forest' in models

    # Check if models have been fit
    for model in models.values():
        assert hasattr(model, 'predict')
        assert hasattr(model, 'fit')

        # Test prediction shape
        predictions = model.predict(sample_data['X_test'])
        assert len(predictions) == len(sample_data['y_test'])

def test_model_performance(sample_data):
    """Test model performance metrics."""
    # Train models
    models = train_models(sample_data['X_train'], sample_data['y_train'])

    # Test each model's performance
    for name, model in models.items():
        # Make predictions
        predictions = model.predict(sample_data['X_test'])

        # Basic performance checks
        assert np.mean(predictions == sample_data['y_test']) > 0.3  # Better than random
        assert len(np.unique(predictions)) == len(np.unique(sample_data['y_test']))

def test_evaluate_models(sample_data, capsys):
    """Test model evaluation function."""
    # Train models
    models = train_models(sample_data['X_train'], sample_data['y_train'])

    # Evaluate models
    evaluate_models(
        models,
        sample_data['X_test'],
        sample_data['y_test'],
        pd.DataFrame(sample_data['X_train'], columns=[f'feature_{i}' for i in range(20)])
    )

    # Check if output contains key metrics
    captured = capsys.readouterr()
    assert 'Classification Report' in captured.out
    assert 'R2 Score' in captured.out

def test_model_reproducibility(sample_data):
    """Test model training reproducibility."""
    # Train models twice
    models1 = train_models(sample_data['X_train'], sample_data['y_train'])
    models2 = train_models(sample_data['X_train'], sample_data['y_train'])

    # Check if predictions are identical
    for name in models1.keys():
        pred1 = models1[name].predict(sample_data['X_test'])
        pred2 = models2[name].predict(sample_data['X_test'])
        assert np.array_equal(pred1, pred2)

def test_invalid_input_handling():
    """Test handling of invalid inputs."""
    # Test with empty arrays
    with pytest.raises(ValueError):
        train_models(np.array([]), np.array([]))

    # Test with mismatched shapes
    X = np.random.rand(10, 5)
    y = np.random.rand(8)
    with pytest.raises(ValueError):
        train_models(X, y)